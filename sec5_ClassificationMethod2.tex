\section{Classifying Users of High Target Specificity}
\label{sec:ClassificationMethod2}

In this chapter, in regards to Twitter users classified into ``the
target specificity is high'' by the method mentioned in the above
chapter, we explain the method of determining why their target
specificities are high based on the result of our analysis mentioned in
\ref{subsec:The Causes}.

Here, we focus on the two causes of the high target specificity
mentioned in \ref{subsec:The Causes} as follows:
\begin{description}
 \item[(1)] because they publish information specified for certain
            topics, and
 \item[(2)] because they publish information to the users specified
            extensionally,
\end{description}

\noindent{and we determine whether a user we intend to classify
correlates with each cause mentioned above.}

We first determine various features of the user which potentially
correlate with each cause.  Then, based on these features, we construct
3-class classifiers which classify users into three categories: (1)
their target specificity is high because they publish information
specified for certain topics, (2) because they publish information to
the users specified extensionally, and (3) in the cause of both (1) and
(2).  By classifying users with the above classifiers, we determine why
their target specificities are high.

{\footnotesize
\begin{figure}[t]
\begin{center}
\includegraphics[width=14cm]{images/classifier.eps}
 \caption{An example of a user's timeline in Twitter}
\label{fig:classifier}
\end{center}
\end{figure}
}

We adopted SVM and decision tree as a classifier.  Next, we explain what
features of users we used for the classification.  All the feature
values shown below were normalized to a value between $0$ and $1$.

\begin{description}
\bf {\item[(i)] numbers of followees and followers, and their ratio}
\end{description}

If the user publishes information to unspecified users, there is high
probability that a number of his/her followers is quite large or a
number of his/her followees is quite small.  So in such a case, a ratio
of a number of his/her followees to a number of his/her followers is
supposed to be very small.  Furthermore, if the user publishes
information to the closed users, i.e., his/her friends, his/her club
members, and so on, there is high probability that numbers of his/her
followers and followees are very close because the user is supposed to
have a reciprocal connection with them.  Thus, numbers of followees and
followers, and their ratio are useful for determining why their target
specificities are high.

\begin{description}
\bf {\item[(ii)] mutual follow ratio}
\end{description}

There is high probability that the user publishing information to the
closed users has a large mutual follow ratio, i.e., a number of users
with whom one follows one another is large, because the user is supposed
to have a reciprocal connection with them.  Therefore, a mutual follow
ratio is expected to be useful for determining why their target
specificities are high.

\begin{description}
\bf {\item[(iii)] frequency of replies by ``@''}
\end{description}

There is high probability that the user publishing information to the
closed users has a high frequency of replies by ``@'', i.e., the user
replies to his/her followers frequently. In regard to a mutual follow
ratio mentioned in (ii), there are some users publishing information to
unspecified users in spite of a large mutual follow ratio, but in regard
to a frequency of replies by ``@'', there is high probability that the
user publish information to the closed users.  This is because a high
frequency of replies by ``@'' demonstrates that the user is supposed to
have a reciprocal connection with them.  Therefore, a frequency of
replies by ``@'' is expected to be useful for determining why their
target specificities are high.

\begin{description}
\bf {\item[(iv)] partialness of topics in messages}
\end{description}

In regard to a user publishing information specified for certain topics,
topics in his/her messages are often partial.  Thus, we use the
partialness of topics in his/her messages as a feature for the
classification.

Then, we explain how to compute a partialness of topics in messages of
$u$.  We first collect up to 200 messages in order of newness.  Second,
we determine the topic of each message by using Latent Dirichlet
Allocation (LDA), which is a generative probabilistic model for
collections of discrete data such as text corpora.  Then, we compute
partialness of topics in messages $\mathit{partialness}(u)$ as follows:

\vspace{-3ex}
\[
 \mathit{partialness}(u) = - \sum_{t \in T_u} p_t \log p_t,\;\;\;\;\;
 p_t = \frac{|\{m|\;m \in M_u,\;\mathit{topic}(m) = t\}|}{|M_u|}
\]
\vspace{-3ex}

\noindent{where $M_u$ is a message set of $u$, $T_u$ is the topic set we
use for computing this feature, and $\mathit{topic}(m)$ is the topic of
a message $m$.  The partialness of topics $partialness(u)$ is the
entropy of $M_u$ on the topic.  This is how we compute a partialness of
topics in messages.}