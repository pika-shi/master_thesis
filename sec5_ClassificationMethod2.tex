\section{Classifying Users of High Target Specificity}
\label{sec:ClassificationMethod2}

In this chapter, in regards to Twitter users classified into ``the
target specificity is high'' by the method mentioned in the above
chapter, we explain the method of determining why their target
specificities are high based on the result of our analysis mentioned in
\ref{subsec:The Causes}.

\subsection{Architecture of the Classifiers}
\label{subsec:Architecture}

Here, we focus on the two causes of the high target specificity
mentioned in \ref{subsec:The Causes} as follows:
\begin{description}
 \item[(1)] because they publish information specified for certain
            topics, and
 \item[(2)] because they publish information to the users specified
            extensionally,
\end{description}

\noindent{and we determine whether a user we intend to classify
correlates with each cause mentioned above.}

We first determine various features of the user which potentially
correlate with each cause.  Then, based on these features, we construct
the classification method which classifies users into three categories:
(1) their target specificity is high because they publish information
specified for certain topics, (2) because they publish information to
the users specified extensionally, and (3) in the cause of both (1) and
(2).  By classifying users with the above classifiers, we determine why
their target specificities are high.

We adopted SVM and decision tree as a classifier.  Next, we propose a
couple of approaches to classify users into three categories.

\subsubsection{3-class Classifier}
\label{subsubsec:3-class}

In the first approach, we construct a single 3-class classifier using
one-against-one method.  Each result class corresponds to each category:
(1), (2), and (3).  Figure~\ref{fig:classifier} (a) shows architecture
of a 3-class classifier.

\subsubsection{2 Binary Classifiers}
\label{subsubsec:2-binary}

In the second approach, we construct two binary classifiers, each of
which determines (i) whether users publish information specified for
certain topics, amd (ii) whether users publish information to the users
specified extensionally, respectively.  When the results of two
classifiers are \emph{yes} and \emph{no} respectively, we classify them
into the category (1): their target specificity is high because they
publish information specified for certain topics.  When \emph{no} and
\emph{yes}, we classify them into the category (2): because they publish
information to the users specified extensionally, and when \emph{yes}
and \emph{yes}, we classify them into the category (3): in the cause of
both (1) and (2).  Figure~\ref{fig:classifier} (b) shows architecture of
2 binary classifiers.

{\footnotesize
\begin{figure}[t]
\begin{center}
\includegraphics[width=14cm]{images/classifier.eps}
 \caption{Architecture of a couple of classifiers: (a) 3-class
 classifier and (b) 2 binary classifiers}
\label{fig:classifier}
\end{center}
\end{figure}
}

\subsection{Features Used for the Classification}
\label{subsec:Features}

In this subchapter, we explain what features of users we used for the
classification.  All the feature values shown below were normalized to
values between $0$ and $1$.

\begin{description}
\bf {\item[(i)] numbers of followees and followers, and their ratio}
\end{description}

If the user publishes information to unspecified users, there is high
probability that a number of his/her followers is quite large or a
number of his/her followees is quite small.  So in such a case, a ratio
of a number of his/her followees to a number of his/her followers is
supposed to be very small.  Furthermore, if the user publishes
information to the closed users, i.e., his/her friends, his/her club
members, and so on, there is high probability that numbers of his/her
followers and followees are very close because the user is supposed to
have a reciprocal connection with them.  Thus, numbers of followees and
followers, and their ratio are expected to be useful for determining why
their target specificities are high.

We take a logarithm of numbers of followers and followees because the
difference of large numbers of followers and followees are not as
important as the difference of small numbers of followers and followees.

\begin{description}
\bf {\item[(ii)] mutual follow ratio}
\end{description}

There is high probability that the user publishing information to the
closed users has a large mutual follow ratio, i.e., a number of users
with whom one follows one another is large, because the user is supposed
to have a reciprocal connection with them.  Therefore, a mutual follow
ratio is expected to be useful for determining why their target
specificities are high.

\begin{description}
\bf {\item[(iii)] frequency of replies by ``@''}
\end{description}

There is high probability that the user publishing information to the
closed users has a high frequency of replies by ``@'', i.e., the user
replies to his/her followers frequently. In regard to a mutual follow
ratio mentioned in (ii), there are some users publishing information to
unspecified users in spite of a large mutual follow ratio, but in regard
to a frequency of replies by ``@'', there is high probability that the
user publish information to the closed users.  This is because a high
frequency of replies by ``@'' demonstrates that the user is supposed to
have a reciprocal connection with them.  Therefore, a frequency of
replies by ``@'' is expected to be useful for determining why their
target specificities are high.

\begin{description}
\bf {\item[(iv)] partialness of topics in messages}
\end{description}

In regard to a user publishing information specified for certain topics,
topics in his/her messages are often partial.  Thus, we use the
partialness of topics in his/her messages as a feature for the
classification.

Then, we explain how to compute a partialness of topics in messages of
$u$.  We first collect up to 200 messages in order of newness.  Second,
we extract noun phrases from them and we use their phrases as a corpus.
We extract only noun phrases bacause they characterize contents of the
messages strongly.  Then, we determine the topic of each message by
using Latent Dirichlet Allocation (LDA), which is a generative
probabilistic model for collections of discrete data such as text
corpora, by using the above corpus.  Finally, we compute partialness of
topics in messages $\mathit{partialness}(u)$ as follows:

\vspace{-3ex}
\[
 \mathit{partialness}(u) = - \sum_{t \in T_u} p_t \log p_t,\;\;\;\;\;
 p_t = \frac{|\{m|\;m \in M_u,\;\mathit{topic}(m) = t\}|}{|M_u|}
\]
\vspace{-3ex}

\noindent{where $M_u$ is a message set of $u$, $T_u$ is the topic set we
use for computing this feature, and $\mathit{topic}(m)$ is the topic of
a message $m$.  The partialness of topics $partialness(u)$ is the
entropy of $M_u$ on the topic.  This is how we compute a partialness of
topics in messages.}