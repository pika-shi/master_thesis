\section{Experiments and Discussion}
\label{sec:Experiment}

In this chapter, we conduct experiments to evaluate our methods proposed
in \ref{sec:ClassificationMethod1} and \ref{sec:ClassificationMethod2},
and present the results obtained from them.  In addition, we discuss our
methods based on the results.

\subsection{Data Set}
\label{subsec:Data Set}

We collected the data set from the real Twitter data by using Twitter
API.

We first randomly selected 1,000 Twitter users whose timezone is Japan.
At this time, we omitted users who are followed from nobody and who post
no tweet in order to select only active users.  Then, we divided them in
two sets equally, i.e., each of which include 500 users.

Second, we had 6 experienced Twitter users as participants, all of whom
are male graduate students in engineering, from 23 to 25 years old.  We
assigned each set to 3 participants, and we asked each participant to
determine one of the following categories each user in the assigned set
is supposed to be in:

\begin{description}
\item[(i)] the user publishes information to the public widely,
\item[(ii)] the user publishes information specified for certain topics,
\item[(iii)] the user publishes information to the users specified
           extensionally, and
\item[(iv)] the user publishes information (ii) specified for certain
           topics (iii) to the users specified extensionally.
\end{description}

\noindent{These categories correspond to the category (b), (1), (2), (3)
in Figure~\ref{fig:Flow} respectively.}

Then, we selected users whose category at least 2 out of 3 participants
coincide with, and as a result, we were able to collect 93, 320, 375,
and 30 users in the category (i), (ii), (iii), and (iv).  We randomly
selected 90 users from the category (i), and 30 users from (ii), (iii),
and (iv) respectively.  We collected these 180 users in total, and we
used them as the data set.  Table~\ref{table:breakdown} shows the
breakdown of the data set: average and standard deviation of numbers of
followers,  followees, and tweets in each category.

\newcolumntype{I}{!{\vrule width 1.5pt}}
\newcommand{\bhline}[1]{\noalign{\hrule height #1}}
\begin{table}[t]
\caption{Average and standard deviation of numbers of followers,
 followees, and tweets in each category \label{table:breakdown}}
\begin{center}
\begin{tabular}{c|cIc|c|c|c}
\multicolumn{2}{cI}{} & \makebox[4em]{i} & \makebox[4em]{ii} &
 \makebox[4em]{iii} & \makebox[4em]{iv} \\ \bhline{1.5pt}
 \multirow{2}{*}{follower} & average & 475,679 & 58,142 & 573 & 82,942  \\
 & standard deviation & 535,894 & 171,784 & 1,389 & 262,161 \\ \hline
 \multirow{2}{*}{followee} & average & 11,274 & 3,353& 598 & 1,568 \\
 & standard deviation & 37,906 & 7,218 & 1,545 & 3,594 \\ \hline
 \multirow{2}{*}{tweet} & average & 9,763 & 9,992 & 8,829 & 5,677 \\
 & standard deviation & 14,607 & 23,572 & 29,505 & 6,600 \\
\end{tabular}
\end{center}
\end{table}

Then, for each user, we collected at most 1,000 followers of the user,
and in regard to the followers who follow at most 1,000 users, we also
collected their followees.  We used them in order to evaluate our
methods.

\subsection{Experimental Settings and Libraries}
\label{subsec:Settings}

First, we conducted the experiments evaluating the method of classifying
Twitter users based on the target specificity of their information
publishing mentioned in \ref{sec:ClassificationMethod1}.  We used 90
users classified into the category (i) as target users, and 90 users
classified into the category (ii), (iii), and (iv) as non target users.
We first computed $\mathit{SpecificityScore}_{{\mathit{term}}}(u)$ and
$\mathit{SpecificityScore}_{{\mathit{followee}}}(u)$ for each user $u$,
and computed $\mathit{TargetSpecificity}(u)$ based on the above scores.
Then, we determined a threshold $\delta$ which can classifies target
users and non target users accurately the most, and evaluated the
classification results with $\delta$.  When computing
$\mathit{TargetSpecificity}(u)$, we used two models: the probabilistic
model and the subtracting model mentioned in \ref{subsec:Scoring}, and
compared them.

Second, we conducted the experiments evaluating the method of determing
why the target specificity of the user is high in regard to the target
user mentioned in \ref{sec:ClassificationMethod2}.  We extracted users
from the category (ii), (iii), and (iv) by 30 users, and used 90 users
in total.  We first extracted features mentioned in
\ref{sec:ClassificationMethod2} from the user, which are normalized to a
value between $0$ and $1$.  Then, based on these features, we
constructed two types of classifiers: a single 3-class classifier and
two binary classifiers, which classify users into three categories:
(ii), (iii), and (iv), and evaluated the classification results using
3-fold cross validation.  We used two learning algorithms: SVM and the
decision tree as classifiers, and compared them.  For SVM, we used
LIBSVM\footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}}, which
is a popular SVM library, with the Gaussian kernel, which using
one-against-one method for multiclass classification.  For the decision
tree, we used scikit-learn
\footnote{\url{http://scikit-learn.org/stable/modules/tree.html}}.

We used twpro search API\footnote{\url{http://twpro.jp/doc/api/search}}
in order to get a number of users who have a certain term in their
profiles.  We also used
MeCab\footnote{\url{http://mecab.sourceforge.net/}} for morphological
analysis of Japanese sentences in profiles, local information, and
tweets of users.  Furthermore, we used
gensim\footnote{\url{http://radimrehurek.com/gensim/}} for using Latent
Dirichlet Allocation (LDA).

\subsection{Results and Evaluation}
\label{subsec:Results}

In this subchapter, we explain the results of our experiments and
evaluate our methods.

\subsubsection{Classifying Users Based on Target Specificity}
\label{subsubsec:Result of Method1}

\subsubsection{Classifying Users of High Target Specificity}
\label{subsubsec:Result of Method2}

We show the results of the experiments classifiying target users.
Table~\ref{table:Precision} shows the precision of four types of
classification methods: a combination of two approaches and two
classifiers, i.e., a 3-class SVM, 2 binary SVMs, a 3-class decision
tree, and 2 binary decision trees, with all features mentioned in
\ref{sec:ClassificationMethod2} into three categories.  The precision
of classification methods using SVMs as classifiers are more than 10
points higher than that using decision trees.  When using SVMs as
classifiers, the precision of 2 binary classifiers are a little higher
than that of a 3-class classifier, and it is the opposite when using
decision trees.  But for both methods using SVMs and decision trees, the
deference between the precision of 2 binary classifiers and that of a
3-class classifier is not so big, which suggests that the two causes of
the high target specificity mentioned in \ref{subsec:The Causes} are
highly independent of each other.

Next, we show the details of the results of classification methods using
SVMs as classifiers.  The second column of Table~\ref{table:Classifier
Details} shows the precision of each classification method with all
features, and the following columns show the precision when we remove
each feature from the data.  Each feature of i, ii, iii, and iv
correspond to those in \ref{sec:ClassificationMethod2} respectively.
For each method, a bold number shows when the precision becomes the
lowest.

The socond and third rows show the precision of a 3-class SVM
and 2 binary SVMs respectively.  For each method, the precision became
the lowest when we removed the feature (i): numbers of followees and
followers, and their ratio.  It is able to be said that numbers of
followees and followers, and their ratio are mainly useful for
determining the cause of high target specificity.  The precision also
became lower to some extent when we removed the feature (iv):
partialness of topics in messages.  This suggests that partialness of
topics in messages is related to high target specificity.
Table~\ref{table:topics} shows a part of topics out of 20 extracted by
LDA.  We were able to extract topics about news programs, disasters, and
so on.  On the other hand, the precision became higher when we　removed
the feature (ii): mutual follow ratio.  This suggests that　mutual
follow ratio and whether the user publishes information to the closed
users are not necessarily correlating.

The fourth and fifth rows show the precision of each binary SVM used for
the method of 2 binary SVMs: a SVM determining whether users publish
information specified for certain topics, and that determining users
publish information to the users specified extensionally, respectively.
The precision of the former became the lowest when we removed the
feature (i): numbers of followees and followers, and their ratio, and
that of the latter became the lowest when we removed the feature
(iv): partialness of topics in messages, which is the noticeable
metter.  This suggests that partialness of topics is more useful for
determing whether users publish information to the users specified
extensionally or not than whether users publish information specified
for certain topics or not.

\begin{table}[t]
\caption{Precision of the classification of target users
 \label{table:Precision}}
 \begin{center}
\begin{tabular}{cIc|c}
 & \makebox[6em]{SVM} & \makebox[6em]{decision tree} \\ \bhline{1.5pt}
 \makebox[10em]{3-class classifier} & 67.8 & 55.6 \\ \hline
 2 binary classifiers & {\bf 68.9} & 53.3 \\
\end{tabular}
 \end{center}
\end{table}

\hdashlinewidth=0.5mm
\begin{table}[t]
\caption{Precision of SVMs without each feature \label{table:Classifier
 Details}}
\begin{center}
\begin{tabular}{ccIc|cccc}
 \multicolumn{2}{cI}{Removed Feature} & \makebox[4em]{with all} &
 \makebox[3em]{i} & \makebox[3em]{ii} & \makebox[3em]{iii} &
 \makebox[3em]{iv} \\ \bhline{1.5pt}
 \multicolumn{2}{cI}{3-class SVM} & 67.8 & {\bf 64.4} & 72.2 & 67.8 &
 65.5 \\ \hline
 \multicolumn{2}{cI}{2 binary SVMs} & 68.9 & {\bf 65.6} & 71.1 & 67.8 &
 67.8 \\ \hdashline
 \makebox[4em]{} & \makebox[3em]{topic} & 85.6 & {\bf 81.1} & 86.7 & 84.4 & 84.4 \\
 \makebox[4em]{} & \makebox[3em]{user} & 83.3 & 84.4 & 84.4 & 83.3 & {\bf 82.2} \\
\end{tabular}
\end{center}
\end{table}

\begin{table}[t]
\caption{A part of topics extracted by LDA
 \label{table:topics}}
\begin{center}
\begin{tabular}{c|c}
topic id & \makebox[25em]{words} \\ \bhline{1.5pt}
1 & news, program, broadcast, morning, night, tonight \\
2 & update, blog, picture, smart phone, weather\\
3 & worst, typhoon, Sea of Japan, electricity \\
4 & earthquake, observation, focus, concern, teacher \\
\end{tabular}
\end{center}
\end{table}